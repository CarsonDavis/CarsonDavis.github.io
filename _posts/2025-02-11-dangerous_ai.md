---
title: The Terror of Progress
date: 2025-01-05
last_modified_at: 2025-01-05
categories: [colllections, old books]
tags: []
description: An Opinion Piece on some of the Dangers of AI
media_subpath: /old-books/
image: old_books_cover.webp
published: False
---

I recently had the chance to be on a podcast with Destin and Matt, where we talked about the state of AI and the bright future or immeninent dangers it might offer. We covered a lot of ground in that podcast, mostly talking about the potentials of AI in a positive light. 

## A Coder's Perspective

In general, my reception to the LLM coding revolution has bene overwhelmingly positive. The easy and power with which we can create new apps using tools like Claude Projects, Windsurf, and Aider is truly amazing, and I think it is no exaggeration to say that it has revolutionized the way we code.

You can read plenty of people on the internet who are very excited about it, and honestly I am too. 

But there are some interesting downsides too. Take the recent Microsoft study [The Impact of Generative AI on Critical Thinking](https://www.microsoft.com/en-us/research/uploads/prod/2025/01/lee_2025_ai_critical_thinking_survey.pdf), which has been hailed in the media with such titles as [AI Makes Human Cognition “Atrophied and Unprepared”](https://www.404media.co/microsoft-study-finds-ai-makes-human-cognition-atrophied-and-unprepared-3/). 

Because modern AI is so powerful, you can sometimes brainlessly get into a loop of having it do ALL the coding for you…solving every error and every architecture decision with another prompt the Machine.

We've noticed that when you rely on it too much, you actually take longer to solve problems and get worse outcomes.

This observation is less true with simple tasks, and more true with grand and complicated architectures.

So anyway, we've noticed this occasional pitfall, and it changes the way at least a couple of us use AI.

My friend is admittedly a little bit of an outlier in many ways, but he will do no-ai-hours… where he forces himself to code everything raw.

I myself use an extensive documentation + AI approach to generate initial projects and features. But after the first flurry of AI creation, I find it very useful to take a step back, carefully read and understand all the code, and (importantly) form strong opinions about the architecture and techniques.

Counterintuitively, I actually work faster when I force myself to do this. And obviously I get better end results too.

Now, to be clear, even with this caveat, AI is a massive net-gain for coding tasks.

But here is the longterm rub…I am able to sit down and critically reexamine the initial collaboration with the AI largely because I have been coding on my own since I was a child. I got to this Senior Dev level by doing Junior Dev tasks for years. By critically thinking and coding my own projects again and again.

So, what happens to the upcoming generation of Junior Devs if they offload all their low-level cognitive tasks to the AI, and they never mentally grow? What happens if there are fewer jobs for Junior Devs, because a Sr Dev has an AI do all their work?

Well, if AI continues to advance at today's pace, then it won’t matter. Eventually it will be good enough that it doesn’t need little ol' me to step in and review its work. We won’t need Junior or Senior devs at all.

But if AI stagnates before reaching this critical phase of eschewing human collaboration, then we run the risk of lobotomizing our talent pipeline at the Junior level, with dangerous results to the supply of Sr devs on 10-15 year time scales.

We might have a double whammy of both fewer junior dev jobs to gain experience in, and the general reduction of junior dev skills for those that have them.

Of course, there is always the counter-example of the calculator’s effect on engineering, but I don’t know how transferable the analogy is, when LMMs are offering to completely replace the human thought process, not to merely supplement it.

## Jenkins Paradox

Actually, I think it's also worth making one clarification on the Jevons Paradox. It can be interpreted as an economic safety blanket, where despite increasing in productivity causing loss of jobs, commensurate increases in demand neutralize the ill effects.
8:49 AM

I've already mentioned in the call that this relies on the two curves growing at similar rates. For example, if demand does grow, but is outstripped by efficiency, you might be in for a lot of unemployed people.
Edited 9:04 AM

But even this threat overlooks the more sinister one of replacement vs improvement. What happens when you aren’t merely improving a worker's efficiency, you are replacing them entirely?
8:52 AM

Well if the changes are slow enough, the societal implications will be minimal. Take for our example groomsmen and the advent of the automobile. Obviously the once ubiquitous jobs of groomsman, farrier, etc are no more, and this collapse of an industry was not accompanied by the collapse of the human economy.
Edited 9:05 AM

But why not? Well, it was two things: slow and partial.

It took decades for cars to fully outphase walking and riding. Decades in which the workforce could adjust.

And it wasn’t every worker. It was only partial replacement of a small subset involved with horses.

This is decidedly not what we potentially face with the threat of AI. This change is not slow and partial. It is fast and comprehensive. It will be upon us in years not decades. And it isn’t affecting one specialized job, it upsets huge swaths of human productivity.
Edited 9:00 AM

So will Jevons Paradox save us?
9:01 AM

Well, it doesn’t matter how much demand increases if you can use electricity to fill that demand, and not a human being.
Edited 9:06 AM

This might come upon us too quickly and too comprehensively, and both society and the economy will be left reeling and unprepared.
9:03 AM